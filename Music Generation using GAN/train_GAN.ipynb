{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "downtown-slope",
   "metadata": {},
   "source": [
    "# Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "balanced-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from tensorflow.keras.layers import Activation, Dense, Bidirectional, LSTM, LeakyReLU, BatchNormalization, Reshape, Input\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "documented-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET = './Dataset/jigs'\n",
    "PATH_TO_MODEL = './Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "apparent-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.listdir(PATH_TO_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liable-wellington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n"
     ]
    }
   ],
   "source": [
    "midi_file_list = glob.glob(PATH_TO_DATASET + '/*.mid')\n",
    "print(len(midi_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "casual-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_notes_from_dataset():\n",
    "    notes = []\n",
    "\n",
    "    for i in tqdm(range(len(midi_file_list)), desc='Parsing MIDI files', ncols = 100):\n",
    "        midi_file = midi_file_list[i]\n",
    "\n",
    "        midi = converter.parse(midi_file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        parts = instrument.partitionByInstrument(midi)\n",
    "\n",
    "        if parts: # file has instrument parts\n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open('notes', 'wb') as filepath:\n",
    "        # write notes in binary format to filepath\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "external-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing MIDI files: 100%|█████████████████████████████████████████| 340/340 [00:51<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of MIDI files in dataset:  340\n",
      "\n",
      "Total number of notes:  85829\n",
      "Total number of unique notes:  75\n"
     ]
    }
   ],
   "source": [
    "notes = extract_notes_from_dataset()\n",
    "\n",
    "print('\\nTotal number of MIDI files in dataset: ', len(midi_file_list))\n",
    "print('\\nTotal number of notes: ', len(notes))\n",
    "print('Total number of unique notes: ', len(set(notes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "northern-awareness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.3.6': 0, '0.3.7': 1, '0.4.7': 2, '0.4.8': 3, '1.4.7': 4, '1.4.7.9': 5, '1.4.8': 6, '10.1.4.6': 7, '10.2.5': 8, '11.2.4.7': 9, '11.2.5.7': 10, '11.2.6': 11, '11.3.6': 12, '2.5.8': 13, '2.5.8.10': 14, '2.5.9': 15, '2.6.9': 16, '3.6.9.11': 17, '3.7.10': 18, '4.7.10': 19, '4.7.10.0': 20, '4.7.11': 21, '4.8.11': 22, '5.8.11': 23, '5.9.0': 24, '6.10.1': 25, '6.9.0.2': 26, '6.9.0.2.3': 27, '6.9.1': 28, '6.9.11': 29, '6.9.11.2': 30, '7.10.1': 31, '7.10.2': 32, '7.11.2': 33, '7.9.1': 34, '8.11.2.4': 35, '8.11.2.4.5': 36, '9.0.3': 37, '9.0.3.5': 38, '9.0.4': 39, '9.1': 40, '9.1.4': 41, '9.11.2.5': 42, 'A3': 43, 'A4': 44, 'A5': 45, 'B-3': 46, 'B-4': 47, 'B-5': 48, 'B3': 49, 'B4': 50, 'B5': 51, 'C#4': 52, 'C#5': 53, 'C#6': 54, 'C4': 55, 'C5': 56, 'C6': 57, 'D4': 58, 'D5': 59, 'D6': 60, 'E-4': 61, 'E-5': 62, 'E4': 63, 'E5': 64, 'F#4': 65, 'F#5': 66, 'F4': 67, 'F5': 68, 'G#3': 69, 'G#4': 70, 'G#5': 71, 'G3': 72, 'G4': 73, 'G5': 74}\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 100\n",
    "\n",
    "unique_notes = sorted(set(notes))\n",
    "unique_notes_count = len(unique_notes)\n",
    "\n",
    "int_to_note = dict((index, note) for index, note in enumerate(unique_notes))\n",
    "note_to_int = dict((note, index) for index, note in enumerate(unique_notes))\n",
    "\n",
    "print(note_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spread-spencer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences:  85729\n",
      "\n",
      "First training sequence:  [66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 66, 50, 11, 53, 50, 50, 35, 66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 53, 59, 64, 16, 66, 64, 59, 35, 53, 50, 44, 41, 53, 64, 45, 66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 66, 50, 11, 53, 50, 50, 35, 66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 53, 59, 64, 16, 66, 64, 59, 35, 53, 50, 44]\n",
      "\n",
      "First target sequence:  41\n"
     ]
    }
   ],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "for i in range(len(notes) - sequence_length):\n",
    "    input_seq = notes[i:i+sequence_length]\n",
    "    target_seq = notes[i+sequence_length]\n",
    "    \n",
    "    input_seq_int = [note_to_int[note] for note in input_seq]\n",
    "    target_seq_int = note_to_int[target_seq]\n",
    "\n",
    "    trainX.append(input_seq_int)\n",
    "    trainY.append(target_seq_int)\n",
    "\n",
    "num_training_seq = len(trainX)\n",
    "\n",
    "print('Number of training sequences: ', num_training_seq)\n",
    "print('\\nFirst training sequence: ', trainX[0])\n",
    "print('\\nFirst target sequence: ', trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "typical-basis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trainX after reshaping (85729, 100, 1)\n",
      "Shape of trainY after one hot encoding:  (85729, 75)\n"
     ]
    }
   ],
   "source": [
    "trainX = np.reshape(trainX, (num_training_seq, sequence_length, 1))\n",
    "print('Shape of trainX after reshaping', trainX.shape)\n",
    "\n",
    "trainX = (trainX - float(unique_notes_count)/2)/(float(unique_notes_count)/2)\n",
    "\n",
    "trainY = to_categorical(trainY)\n",
    "print('Shape of trainY after one hot encoding: ', trainY.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-enemy",
   "metadata": {},
   "source": [
    "# **MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "overall-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 32\n",
    "learning_rate = 2e-4\n",
    "seq_length = 100\n",
    "latent_dim = 1000\n",
    "seq_shape = (seq_length, 1)\n",
    "generator_loss = []\n",
    "discriminator_loss = []\n",
    "\n",
    "optimizer = Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bulgarian-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=seq_shape, return_sequences=True))\n",
    "    model.add(Bidirectional(LSTM(512)))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print('\\nDiscriminator model: \\n')\n",
    "    model.summary()\n",
    "\n",
    "    input_seq = Input(shape=seq_shape)\n",
    "    output_seq = model(input_seq)\n",
    "\n",
    "    return Model(input_seq, output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rotary-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(seq_shape), activation='tanh'))\n",
    "    model.add(Reshape(seq_shape))\n",
    "    \n",
    "    print('\\nGenerator model: \\n')\n",
    "    model.summary()\n",
    "    \n",
    "    input_noise = Input(shape=(latent_dim,))\n",
    "    output_seq = model(input_noise)\n",
    "\n",
    "    return Model(input_noise, output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "french-oxford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discriminator model: \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 1024)              4198400   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,907,457\n",
      "Trainable params: 5,907,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Generator model: \n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 256)               256256    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               102500    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 100, 1)            0         \n",
      "=================================================================\n",
      "Total params: 1,022,820\n",
      "Trainable params: 1,019,236\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Take output from generator after feeding it with noise\n",
    "input_seq = Input(shape=(latent_dim,))\n",
    "generated_seq = generator(input_seq)\n",
    "\n",
    "# For combined model only generator should be trained\n",
    "discriminator.trainable = False\n",
    "\n",
    "output_seq = discriminator(generated_seq)\n",
    "\n",
    "combinedModel = Model(input_seq, output_seq)\n",
    "combinedModel.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "thick-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAN():\n",
    "    global notes, unique_notes_count, trainX, trainY, epochs, batch_size, latent_dim, discriminator_loss, generator_loss\n",
    "    \n",
    "    discriminator_loss = []\n",
    "    generator_loss = []\n",
    "    \n",
    "    real = np.ones((batch_size,1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    start_time = timer()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Choosing a batch of sequences randomly to train model\n",
    "        seq_indexes = np.random.randint(0, len(trainX), batch_size)\n",
    "        real_seq_batch = trainX[seq_indexes]\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        generated_seq_batch = generator.predict(noise)\n",
    "        \n",
    "        # Training discriminator\n",
    "        disc_loss_real = discriminator.train_on_batch(real_seq_batch, real)\n",
    "        disc_loss_fake = discriminator.train_on_batch(generated_seq_batch, fake)\n",
    "        \n",
    "        disc_loss = np.add(disc_loss_real, disc_loss_fake) * 0.5\n",
    "        \n",
    "        # Training generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_loss = combinedModel.train_on_batch(noise, real)\n",
    "        \n",
    "        if epoch == 0 or (epoch+1) % 5 == 0:\n",
    "            print('EPOCH: {:<4} / {} \\t DISC_LOSS: {:.2f} \\t DISC_ACC: {:.2f} \\t GEN_LOSS: {:.2f}'.format(epoch+1, epochs, disc_loss[0], disc_loss[1], gen_loss))\n",
    "            discriminator_loss.append(disc_loss[0])\n",
    "            generator_loss.append(gen_loss)\n",
    "            \n",
    "    end_time = timer()\n",
    "    elapsed_time = '{:.2f}s'.format(end_time - start_time)\n",
    "    print('\\nTota time elapsed: ', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "expired-client",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1    / 1000 \t DISC_LOSS: 0.70 \t DISC_ACC: 0.00 \t GEN_LOSS: 0.69\n",
      "EPOCH: 5    / 1000 \t DISC_LOSS: 0.60 \t DISC_ACC: 0.70 \t GEN_LOSS: 0.67\n",
      "EPOCH: 10   / 1000 \t DISC_LOSS: 0.27 \t DISC_ACC: 0.91 \t GEN_LOSS: 0.92\n",
      "EPOCH: 15   / 1000 \t DISC_LOSS: 0.16 \t DISC_ACC: 0.98 \t GEN_LOSS: 7.47\n",
      "EPOCH: 20   / 1000 \t DISC_LOSS: 0.09 \t DISC_ACC: 0.98 \t GEN_LOSS: 8.08\n",
      "EPOCH: 25   / 1000 \t DISC_LOSS: 0.11 \t DISC_ACC: 0.98 \t GEN_LOSS: 8.76\n",
      "EPOCH: 30   / 1000 \t DISC_LOSS: 0.26 \t DISC_ACC: 0.94 \t GEN_LOSS: 3.89\n",
      "EPOCH: 35   / 1000 \t DISC_LOSS: 0.11 \t DISC_ACC: 0.95 \t GEN_LOSS: 9.04\n",
      "EPOCH: 40   / 1000 \t DISC_LOSS: 0.41 \t DISC_ACC: 0.86 \t GEN_LOSS: 13.56\n",
      "EPOCH: 45   / 1000 \t DISC_LOSS: 0.27 \t DISC_ACC: 0.91 \t GEN_LOSS: 16.81\n",
      "EPOCH: 50   / 1000 \t DISC_LOSS: 0.16 \t DISC_ACC: 0.97 \t GEN_LOSS: 8.42\n",
      "EPOCH: 55   / 1000 \t DISC_LOSS: 0.23 \t DISC_ACC: 0.94 \t GEN_LOSS: 7.68\n",
      "EPOCH: 60   / 1000 \t DISC_LOSS: 0.28 \t DISC_ACC: 0.89 \t GEN_LOSS: 18.56\n",
      "EPOCH: 65   / 1000 \t DISC_LOSS: 0.08 \t DISC_ACC: 0.97 \t GEN_LOSS: 26.04\n",
      "EPOCH: 70   / 1000 \t DISC_LOSS: 0.39 \t DISC_ACC: 0.84 \t GEN_LOSS: 25.52\n",
      "EPOCH: 75   / 1000 \t DISC_LOSS: 0.44 \t DISC_ACC: 0.81 \t GEN_LOSS: 12.07\n",
      "EPOCH: 80   / 1000 \t DISC_LOSS: 0.35 \t DISC_ACC: 0.91 \t GEN_LOSS: 3.31\n",
      "EPOCH: 85   / 1000 \t DISC_LOSS: 0.22 \t DISC_ACC: 0.94 \t GEN_LOSS: 3.89\n",
      "EPOCH: 90   / 1000 \t DISC_LOSS: 0.12 \t DISC_ACC: 0.97 \t GEN_LOSS: 7.05\n",
      "EPOCH: 95   / 1000 \t DISC_LOSS: 0.31 \t DISC_ACC: 0.91 \t GEN_LOSS: 5.81\n",
      "EPOCH: 100  / 1000 \t DISC_LOSS: 0.43 \t DISC_ACC: 0.81 \t GEN_LOSS: 5.19\n",
      "EPOCH: 105  / 1000 \t DISC_LOSS: 0.29 \t DISC_ACC: 0.92 \t GEN_LOSS: 2.99\n",
      "EPOCH: 110  / 1000 \t DISC_LOSS: 0.37 \t DISC_ACC: 0.84 \t GEN_LOSS: 3.29\n",
      "EPOCH: 115  / 1000 \t DISC_LOSS: 0.33 \t DISC_ACC: 0.88 \t GEN_LOSS: 3.13\n",
      "EPOCH: 120  / 1000 \t DISC_LOSS: 0.25 \t DISC_ACC: 0.89 \t GEN_LOSS: 2.99\n",
      "EPOCH: 125  / 1000 \t DISC_LOSS: 0.20 \t DISC_ACC: 0.95 \t GEN_LOSS: 2.91\n",
      "EPOCH: 130  / 1000 \t DISC_LOSS: 0.11 \t DISC_ACC: 0.97 \t GEN_LOSS: 3.01\n",
      "EPOCH: 135  / 1000 \t DISC_LOSS: 0.37 \t DISC_ACC: 0.86 \t GEN_LOSS: 2.62\n",
      "EPOCH: 140  / 1000 \t DISC_LOSS: 0.46 \t DISC_ACC: 0.86 \t GEN_LOSS: 4.98\n",
      "EPOCH: 145  / 1000 \t DISC_LOSS: 0.23 \t DISC_ACC: 0.92 \t GEN_LOSS: 5.24\n",
      "EPOCH: 150  / 1000 \t DISC_LOSS: 0.56 \t DISC_ACC: 0.69 \t GEN_LOSS: 1.92\n",
      "EPOCH: 155  / 1000 \t DISC_LOSS: 0.57 \t DISC_ACC: 0.69 \t GEN_LOSS: 3.26\n",
      "EPOCH: 160  / 1000 \t DISC_LOSS: 0.66 \t DISC_ACC: 0.72 \t GEN_LOSS: 5.41\n",
      "EPOCH: 165  / 1000 \t DISC_LOSS: 0.45 \t DISC_ACC: 0.80 \t GEN_LOSS: 4.60\n",
      "EPOCH: 170  / 1000 \t DISC_LOSS: 0.40 \t DISC_ACC: 0.80 \t GEN_LOSS: 6.24\n",
      "EPOCH: 175  / 1000 \t DISC_LOSS: 0.33 \t DISC_ACC: 0.84 \t GEN_LOSS: 8.87\n",
      "EPOCH: 180  / 1000 \t DISC_LOSS: 0.24 \t DISC_ACC: 0.91 \t GEN_LOSS: 9.23\n",
      "EPOCH: 185  / 1000 \t DISC_LOSS: 0.25 \t DISC_ACC: 0.95 \t GEN_LOSS: 4.95\n",
      "EPOCH: 190  / 1000 \t DISC_LOSS: 0.38 \t DISC_ACC: 0.83 \t GEN_LOSS: 3.31\n",
      "EPOCH: 195  / 1000 \t DISC_LOSS: 0.38 \t DISC_ACC: 0.86 \t GEN_LOSS: 4.18\n",
      "EPOCH: 200  / 1000 \t DISC_LOSS: 0.37 \t DISC_ACC: 0.83 \t GEN_LOSS: 4.77\n",
      "EPOCH: 205  / 1000 \t DISC_LOSS: 0.33 \t DISC_ACC: 0.88 \t GEN_LOSS: 5.18\n",
      "EPOCH: 210  / 1000 \t DISC_LOSS: 0.56 \t DISC_ACC: 0.80 \t GEN_LOSS: 5.02\n",
      "EPOCH: 215  / 1000 \t DISC_LOSS: 0.42 \t DISC_ACC: 0.80 \t GEN_LOSS: 3.56\n",
      "EPOCH: 220  / 1000 \t DISC_LOSS: 0.44 \t DISC_ACC: 0.81 \t GEN_LOSS: 4.14\n",
      "EPOCH: 225  / 1000 \t DISC_LOSS: 0.33 \t DISC_ACC: 0.86 \t GEN_LOSS: 4.33\n",
      "EPOCH: 230  / 1000 \t DISC_LOSS: 0.33 \t DISC_ACC: 0.83 \t GEN_LOSS: 3.79\n",
      "EPOCH: 235  / 1000 \t DISC_LOSS: 0.37 \t DISC_ACC: 0.81 \t GEN_LOSS: 3.88\n",
      "EPOCH: 240  / 1000 \t DISC_LOSS: 0.46 \t DISC_ACC: 0.81 \t GEN_LOSS: 2.17\n",
      "EPOCH: 245  / 1000 \t DISC_LOSS: 0.27 \t DISC_ACC: 0.89 \t GEN_LOSS: 3.19\n",
      "EPOCH: 250  / 1000 \t DISC_LOSS: 0.28 \t DISC_ACC: 0.86 \t GEN_LOSS: 3.94\n",
      "EPOCH: 255  / 1000 \t DISC_LOSS: 0.18 \t DISC_ACC: 0.95 \t GEN_LOSS: 5.00\n",
      "EPOCH: 260  / 1000 \t DISC_LOSS: 0.25 \t DISC_ACC: 0.91 \t GEN_LOSS: 5.84\n",
      "EPOCH: 265  / 1000 \t DISC_LOSS: 0.12 \t DISC_ACC: 0.97 \t GEN_LOSS: 8.07\n",
      "EPOCH: 270  / 1000 \t DISC_LOSS: 0.09 \t DISC_ACC: 0.95 \t GEN_LOSS: 9.79\n",
      "EPOCH: 275  / 1000 \t DISC_LOSS: 1.77 \t DISC_ACC: 0.75 \t GEN_LOSS: 8.23\n",
      "EPOCH: 280  / 1000 \t DISC_LOSS: 0.21 \t DISC_ACC: 0.94 \t GEN_LOSS: 3.02\n",
      "EPOCH: 285  / 1000 \t DISC_LOSS: 0.17 \t DISC_ACC: 0.95 \t GEN_LOSS: 4.13\n",
      "EPOCH: 290  / 1000 \t DISC_LOSS: 0.15 \t DISC_ACC: 0.94 \t GEN_LOSS: 6.82\n",
      "EPOCH: 295  / 1000 \t DISC_LOSS: 0.01 \t DISC_ACC: 1.00 \t GEN_LOSS: 6.14\n",
      "EPOCH: 300  / 1000 \t DISC_LOSS: 0.49 \t DISC_ACC: 0.86 \t GEN_LOSS: 4.13\n",
      "EPOCH: 305  / 1000 \t DISC_LOSS: 0.37 \t DISC_ACC: 0.86 \t GEN_LOSS: 4.16\n",
      "EPOCH: 310  / 1000 \t DISC_LOSS: 0.27 \t DISC_ACC: 0.91 \t GEN_LOSS: 5.01\n",
      "EPOCH: 315  / 1000 \t DISC_LOSS: 0.31 \t DISC_ACC: 0.89 \t GEN_LOSS: 5.92\n",
      "EPOCH: 320  / 1000 \t DISC_LOSS: 0.30 \t DISC_ACC: 0.91 \t GEN_LOSS: 6.11\n",
      "EPOCH: 325  / 1000 \t DISC_LOSS: 0.25 \t DISC_ACC: 0.89 \t GEN_LOSS: 4.69\n",
      "EPOCH: 330  / 1000 \t DISC_LOSS: 0.30 \t DISC_ACC: 0.86 \t GEN_LOSS: 6.20\n",
      "EPOCH: 335  / 1000 \t DISC_LOSS: 0.34 \t DISC_ACC: 0.84 \t GEN_LOSS: 5.61\n",
      "EPOCH: 340  / 1000 \t DISC_LOSS: 0.15 \t DISC_ACC: 0.95 \t GEN_LOSS: 5.15\n",
      "EPOCH: 345  / 1000 \t DISC_LOSS: 0.13 \t DISC_ACC: 0.97 \t GEN_LOSS: 8.10\n",
      "EPOCH: 350  / 1000 \t DISC_LOSS: 0.13 \t DISC_ACC: 0.97 \t GEN_LOSS: 8.68\n",
      "EPOCH: 355  / 1000 \t DISC_LOSS: 0.08 \t DISC_ACC: 0.95 \t GEN_LOSS: 9.58\n",
      "EPOCH: 360  / 1000 \t DISC_LOSS: 0.29 \t DISC_ACC: 0.84 \t GEN_LOSS: 7.91\n",
      "EPOCH: 365  / 1000 \t DISC_LOSS: 0.16 \t DISC_ACC: 0.94 \t GEN_LOSS: 7.93\n",
      "EPOCH: 370  / 1000 \t DISC_LOSS: 0.15 \t DISC_ACC: 0.94 \t GEN_LOSS: 10.04\n",
      "EPOCH: 375  / 1000 \t DISC_LOSS: 0.13 \t DISC_ACC: 0.95 \t GEN_LOSS: 10.54\n",
      "EPOCH: 380  / 1000 \t DISC_LOSS: 0.51 \t DISC_ACC: 0.86 \t GEN_LOSS: 6.94\n",
      "EPOCH: 385  / 1000 \t DISC_LOSS: 0.35 \t DISC_ACC: 0.86 \t GEN_LOSS: 4.94\n",
      "EPOCH: 390  / 1000 \t DISC_LOSS: 0.45 \t DISC_ACC: 0.88 \t GEN_LOSS: 3.87\n",
      "EPOCH: 395  / 1000 \t DISC_LOSS: 0.25 \t DISC_ACC: 0.92 \t GEN_LOSS: 3.91\n",
      "EPOCH: 400  / 1000 \t DISC_LOSS: 0.32 \t DISC_ACC: 0.91 \t GEN_LOSS: 2.81\n",
      "EPOCH: 405  / 1000 \t DISC_LOSS: 0.36 \t DISC_ACC: 0.88 \t GEN_LOSS: 2.83\n",
      "EPOCH: 410  / 1000 \t DISC_LOSS: 0.12 \t DISC_ACC: 0.97 \t GEN_LOSS: 3.51\n",
      "EPOCH: 415  / 1000 \t DISC_LOSS: 0.34 \t DISC_ACC: 0.91 \t GEN_LOSS: 4.72\n",
      "EPOCH: 420  / 1000 \t DISC_LOSS: 0.24 \t DISC_ACC: 0.91 \t GEN_LOSS: 3.96\n",
      "EPOCH: 425  / 1000 \t DISC_LOSS: 0.24 \t DISC_ACC: 0.94 \t GEN_LOSS: 4.29\n",
      "EPOCH: 430  / 1000 \t DISC_LOSS: 0.08 \t DISC_ACC: 0.98 \t GEN_LOSS: 4.38\n",
      "EPOCH: 435  / 1000 \t DISC_LOSS: 0.17 \t DISC_ACC: 0.95 \t GEN_LOSS: 3.53\n",
      "EPOCH: 440  / 1000 \t DISC_LOSS: 0.23 \t DISC_ACC: 0.89 \t GEN_LOSS: 4.06\n",
      "EPOCH: 445  / 1000 \t DISC_LOSS: 0.04 \t DISC_ACC: 0.98 \t GEN_LOSS: 5.25\n",
      "EPOCH: 450  / 1000 \t DISC_LOSS: 0.05 \t DISC_ACC: 0.98 \t GEN_LOSS: 4.66\n",
      "EPOCH: 455  / 1000 \t DISC_LOSS: 0.05 \t DISC_ACC: 1.00 \t GEN_LOSS: 4.94\n",
      "EPOCH: 460  / 1000 \t DISC_LOSS: 0.07 \t DISC_ACC: 0.98 \t GEN_LOSS: 5.94\n",
      "EPOCH: 465  / 1000 \t DISC_LOSS: 0.17 \t DISC_ACC: 0.92 \t GEN_LOSS: 4.56\n",
      "EPOCH: 470  / 1000 \t DISC_LOSS: 0.16 \t DISC_ACC: 0.97 \t GEN_LOSS: 3.70\n",
      "EPOCH: 475  / 1000 \t DISC_LOSS: 0.30 \t DISC_ACC: 0.88 \t GEN_LOSS: 6.30\n",
      "EPOCH: 480  / 1000 \t DISC_LOSS: 0.45 \t DISC_ACC: 0.81 \t GEN_LOSS: 3.93\n",
      "EPOCH: 485  / 1000 \t DISC_LOSS: 0.37 \t DISC_ACC: 0.86 \t GEN_LOSS: 3.68\n",
      "EPOCH: 490  / 1000 \t DISC_LOSS: 0.33 \t DISC_ACC: 0.84 \t GEN_LOSS: 4.26\n",
      "EPOCH: 495  / 1000 \t DISC_LOSS: 0.22 \t DISC_ACC: 0.94 \t GEN_LOSS: 6.12\n",
      "EPOCH: 500  / 1000 \t DISC_LOSS: 0.33 \t DISC_ACC: 0.92 \t GEN_LOSS: 6.28\n",
      "EPOCH: 505  / 1000 \t DISC_LOSS: 0.10 \t DISC_ACC: 0.97 \t GEN_LOSS: 7.12\n",
      "EPOCH: 510  / 1000 \t DISC_LOSS: 0.39 \t DISC_ACC: 0.92 \t GEN_LOSS: 6.23\n",
      "EPOCH: 515  / 1000 \t DISC_LOSS: 0.20 \t DISC_ACC: 0.91 \t GEN_LOSS: 5.01\n",
      "EPOCH: 520  / 1000 \t DISC_LOSS: 0.34 \t DISC_ACC: 0.84 \t GEN_LOSS: 3.46\n",
      "EPOCH: 525  / 1000 \t DISC_LOSS: 0.26 \t DISC_ACC: 0.91 \t GEN_LOSS: 3.91\n",
      "EPOCH: 530  / 1000 \t DISC_LOSS: 0.10 \t DISC_ACC: 0.97 \t GEN_LOSS: 4.37\n",
      "EPOCH: 535  / 1000 \t DISC_LOSS: 0.14 \t DISC_ACC: 0.95 \t GEN_LOSS: 5.67\n",
      "EPOCH: 540  / 1000 \t DISC_LOSS: 0.07 \t DISC_ACC: 0.98 \t GEN_LOSS: 4.92\n",
      "EPOCH: 545  / 1000 \t DISC_LOSS: 0.18 \t DISC_ACC: 0.97 \t GEN_LOSS: 5.10\n",
      "EPOCH: 550  / 1000 \t DISC_LOSS: 0.26 \t DISC_ACC: 0.89 \t GEN_LOSS: 5.32\n",
      "EPOCH: 555  / 1000 \t DISC_LOSS: 0.18 \t DISC_ACC: 0.94 \t GEN_LOSS: 4.94\n",
      "EPOCH: 560  / 1000 \t DISC_LOSS: 0.09 \t DISC_ACC: 0.97 \t GEN_LOSS: 4.06\n",
      "EPOCH: 565  / 1000 \t DISC_LOSS: 0.08 \t DISC_ACC: 0.97 \t GEN_LOSS: 5.36\n",
      "EPOCH: 570  / 1000 \t DISC_LOSS: 0.34 \t DISC_ACC: 0.89 \t GEN_LOSS: 5.31\n",
      "EPOCH: 575  / 1000 \t DISC_LOSS: 0.22 \t DISC_ACC: 0.92 \t GEN_LOSS: 3.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 580  / 1000 \t DISC_LOSS: 0.20 \t DISC_ACC: 0.94 \t GEN_LOSS: 4.44\n",
      "EPOCH: 585  / 1000 \t DISC_LOSS: 0.25 \t DISC_ACC: 0.89 \t GEN_LOSS: 4.17\n",
      "EPOCH: 590  / 1000 \t DISC_LOSS: 0.23 \t DISC_ACC: 0.91 \t GEN_LOSS: 3.62\n",
      "EPOCH: 595  / 1000 \t DISC_LOSS: 0.17 \t DISC_ACC: 0.94 \t GEN_LOSS: 4.63\n",
      "EPOCH: 600  / 1000 \t DISC_LOSS: 0.20 \t DISC_ACC: 0.94 \t GEN_LOSS: 4.94\n",
      "EPOCH: 605  / 1000 \t DISC_LOSS: 0.07 \t DISC_ACC: 0.98 \t GEN_LOSS: 3.34\n",
      "EPOCH: 610  / 1000 \t DISC_LOSS: 0.20 \t DISC_ACC: 0.92 \t GEN_LOSS: 4.43\n",
      "EPOCH: 615  / 1000 \t DISC_LOSS: 0.14 \t DISC_ACC: 0.95 \t GEN_LOSS: 4.05\n",
      "EPOCH: 620  / 1000 \t DISC_LOSS: 0.26 \t DISC_ACC: 0.91 \t GEN_LOSS: 4.11\n",
      "EPOCH: 625  / 1000 \t DISC_LOSS: 0.14 \t DISC_ACC: 0.95 \t GEN_LOSS: 4.60\n",
      "EPOCH: 630  / 1000 \t DISC_LOSS: 0.17 \t DISC_ACC: 0.94 \t GEN_LOSS: 4.41\n",
      "EPOCH: 635  / 1000 \t DISC_LOSS: 0.17 \t DISC_ACC: 0.94 \t GEN_LOSS: 3.74\n",
      "EPOCH: 640  / 1000 \t DISC_LOSS: 0.24 \t DISC_ACC: 0.92 \t GEN_LOSS: 3.75\n",
      "EPOCH: 645  / 1000 \t DISC_LOSS: 0.11 \t DISC_ACC: 0.97 \t GEN_LOSS: 3.93\n",
      "EPOCH: 650  / 1000 \t DISC_LOSS: 0.17 \t DISC_ACC: 0.92 \t GEN_LOSS: 3.68\n",
      "EPOCH: 655  / 1000 \t DISC_LOSS: 0.18 \t DISC_ACC: 0.92 \t GEN_LOSS: 4.14\n",
      "EPOCH: 660  / 1000 \t DISC_LOSS: 0.20 \t DISC_ACC: 0.91 \t GEN_LOSS: 4.23\n",
      "EPOCH: 665  / 1000 \t DISC_LOSS: 0.12 \t DISC_ACC: 0.97 \t GEN_LOSS: 4.80\n",
      "EPOCH: 670  / 1000 \t DISC_LOSS: 0.14 \t DISC_ACC: 0.95 \t GEN_LOSS: 3.47\n",
      "EPOCH: 675  / 1000 \t DISC_LOSS: 0.26 \t DISC_ACC: 0.88 \t GEN_LOSS: 2.91\n",
      "EPOCH: 680  / 1000 \t DISC_LOSS: 0.19 \t DISC_ACC: 0.91 \t GEN_LOSS: 3.13\n",
      "EPOCH: 685  / 1000 \t DISC_LOSS: 0.21 \t DISC_ACC: 0.89 \t GEN_LOSS: 3.77\n",
      "EPOCH: 690  / 1000 \t DISC_LOSS: 0.11 \t DISC_ACC: 0.97 \t GEN_LOSS: 3.62\n",
      "EPOCH: 695  / 1000 \t DISC_LOSS: 0.27 \t DISC_ACC: 0.88 \t GEN_LOSS: 4.05\n",
      "EPOCH: 700  / 1000 \t DISC_LOSS: 0.27 \t DISC_ACC: 0.89 \t GEN_LOSS: 3.10\n",
      "EPOCH: 705  / 1000 \t DISC_LOSS: 0.20 \t DISC_ACC: 0.88 \t GEN_LOSS: 2.65\n",
      "EPOCH: 710  / 1000 \t DISC_LOSS: 0.16 \t DISC_ACC: 0.97 \t GEN_LOSS: 4.20\n",
      "EPOCH: 715  / 1000 \t DISC_LOSS: 0.13 \t DISC_ACC: 0.95 \t GEN_LOSS: 4.21\n",
      "EPOCH: 720  / 1000 \t DISC_LOSS: 0.18 \t DISC_ACC: 0.97 \t GEN_LOSS: 3.77\n",
      "EPOCH: 725  / 1000 \t DISC_LOSS: 0.11 \t DISC_ACC: 0.95 \t GEN_LOSS: 3.91\n",
      "EPOCH: 730  / 1000 \t DISC_LOSS: 0.18 \t DISC_ACC: 0.91 \t GEN_LOSS: 4.31\n",
      "EPOCH: 735  / 1000 \t DISC_LOSS: 0.12 \t DISC_ACC: 0.94 \t GEN_LOSS: 4.67\n",
      "EPOCH: 740  / 1000 \t DISC_LOSS: 0.31 \t DISC_ACC: 0.89 \t GEN_LOSS: 2.76\n",
      "EPOCH: 745  / 1000 \t DISC_LOSS: 0.25 \t DISC_ACC: 0.89 \t GEN_LOSS: 3.19\n",
      "EPOCH: 750  / 1000 \t DISC_LOSS: 0.10 \t DISC_ACC: 0.97 \t GEN_LOSS: 4.05\n",
      "EPOCH: 755  / 1000 \t DISC_LOSS: 0.11 \t DISC_ACC: 0.97 \t GEN_LOSS: 4.77\n",
      "EPOCH: 760  / 1000 \t DISC_LOSS: 0.17 \t DISC_ACC: 0.92 \t GEN_LOSS: 4.79\n",
      "EPOCH: 765  / 1000 \t DISC_LOSS: 0.23 \t DISC_ACC: 0.91 \t GEN_LOSS: 5.19\n",
      "EPOCH: 770  / 1000 \t DISC_LOSS: 0.23 \t DISC_ACC: 0.94 \t GEN_LOSS: 5.67\n",
      "EPOCH: 775  / 1000 \t DISC_LOSS: 0.09 \t DISC_ACC: 0.98 \t GEN_LOSS: 4.46\n",
      "EPOCH: 780  / 1000 \t DISC_LOSS: 0.04 \t DISC_ACC: 1.00 \t GEN_LOSS: 5.83\n",
      "EPOCH: 785  / 1000 \t DISC_LOSS: 0.01 \t DISC_ACC: 1.00 \t GEN_LOSS: 7.75\n",
      "EPOCH: 790  / 1000 \t DISC_LOSS: 0.24 \t DISC_ACC: 0.91 \t GEN_LOSS: 2.56\n",
      "EPOCH: 795  / 1000 \t DISC_LOSS: 0.29 \t DISC_ACC: 0.94 \t GEN_LOSS: 2.07\n",
      "EPOCH: 800  / 1000 \t DISC_LOSS: 0.31 \t DISC_ACC: 0.91 \t GEN_LOSS: 2.90\n",
      "EPOCH: 805  / 1000 \t DISC_LOSS: 0.11 \t DISC_ACC: 0.97 \t GEN_LOSS: 5.20\n",
      "EPOCH: 810  / 1000 \t DISC_LOSS: 0.21 \t DISC_ACC: 0.95 \t GEN_LOSS: 7.05\n",
      "EPOCH: 815  / 1000 \t DISC_LOSS: 0.04 \t DISC_ACC: 0.98 \t GEN_LOSS: 6.57\n",
      "EPOCH: 820  / 1000 \t DISC_LOSS: 0.03 \t DISC_ACC: 1.00 \t GEN_LOSS: 6.06\n",
      "EPOCH: 825  / 1000 \t DISC_LOSS: 0.05 \t DISC_ACC: 0.98 \t GEN_LOSS: 5.76\n",
      "EPOCH: 830  / 1000 \t DISC_LOSS: 0.06 \t DISC_ACC: 0.98 \t GEN_LOSS: 5.77\n",
      "EPOCH: 835  / 1000 \t DISC_LOSS: 0.14 \t DISC_ACC: 0.98 \t GEN_LOSS: 7.02\n",
      "EPOCH: 840  / 1000 \t DISC_LOSS: 0.01 \t DISC_ACC: 1.00 \t GEN_LOSS: 5.45\n",
      "EPOCH: 845  / 1000 \t DISC_LOSS: 0.29 \t DISC_ACC: 0.86 \t GEN_LOSS: 3.06\n",
      "EPOCH: 850  / 1000 \t DISC_LOSS: 0.20 \t DISC_ACC: 0.95 \t GEN_LOSS: 2.53\n",
      "EPOCH: 855  / 1000 \t DISC_LOSS: 0.08 \t DISC_ACC: 1.00 \t GEN_LOSS: 4.76\n",
      "EPOCH: 860  / 1000 \t DISC_LOSS: 0.08 \t DISC_ACC: 0.98 \t GEN_LOSS: 5.80\n",
      "EPOCH: 865  / 1000 \t DISC_LOSS: 0.04 \t DISC_ACC: 0.98 \t GEN_LOSS: 7.00\n",
      "EPOCH: 870  / 1000 \t DISC_LOSS: 0.01 \t DISC_ACC: 1.00 \t GEN_LOSS: 6.29\n",
      "EPOCH: 875  / 1000 \t DISC_LOSS: 0.01 \t DISC_ACC: 1.00 \t GEN_LOSS: 5.66\n",
      "EPOCH: 880  / 1000 \t DISC_LOSS: 0.03 \t DISC_ACC: 0.98 \t GEN_LOSS: 5.58\n",
      "EPOCH: 885  / 1000 \t DISC_LOSS: 0.01 \t DISC_ACC: 1.00 \t GEN_LOSS: 5.39\n",
      "EPOCH: 890  / 1000 \t DISC_LOSS: 0.09 \t DISC_ACC: 0.98 \t GEN_LOSS: 5.68\n",
      "EPOCH: 895  / 1000 \t DISC_LOSS: 0.10 \t DISC_ACC: 0.94 \t GEN_LOSS: 4.91\n",
      "EPOCH: 900  / 1000 \t DISC_LOSS: 0.01 \t DISC_ACC: 1.00 \t GEN_LOSS: 4.76\n",
      "EPOCH: 905  / 1000 \t DISC_LOSS: 0.05 \t DISC_ACC: 0.98 \t GEN_LOSS: 5.72\n",
      "EPOCH: 910  / 1000 \t DISC_LOSS: 0.19 \t DISC_ACC: 0.92 \t GEN_LOSS: 2.73\n",
      "EPOCH: 915  / 1000 \t DISC_LOSS: 0.05 \t DISC_ACC: 1.00 \t GEN_LOSS: 3.54\n",
      "EPOCH: 920  / 1000 \t DISC_LOSS: 0.16 \t DISC_ACC: 0.94 \t GEN_LOSS: 5.89\n",
      "EPOCH: 925  / 1000 \t DISC_LOSS: 0.20 \t DISC_ACC: 0.95 \t GEN_LOSS: 5.51\n",
      "EPOCH: 930  / 1000 \t DISC_LOSS: 0.03 \t DISC_ACC: 0.98 \t GEN_LOSS: 5.73\n",
      "EPOCH: 935  / 1000 \t DISC_LOSS: 0.15 \t DISC_ACC: 0.94 \t GEN_LOSS: 3.67\n",
      "EPOCH: 940  / 1000 \t DISC_LOSS: 0.16 \t DISC_ACC: 0.91 \t GEN_LOSS: 5.09\n",
      "EPOCH: 945  / 1000 \t DISC_LOSS: 0.02 \t DISC_ACC: 1.00 \t GEN_LOSS: 5.23\n",
      "EPOCH: 950  / 1000 \t DISC_LOSS: 0.15 \t DISC_ACC: 0.95 \t GEN_LOSS: 5.48\n",
      "EPOCH: 955  / 1000 \t DISC_LOSS: 0.14 \t DISC_ACC: 0.95 \t GEN_LOSS: 4.42\n",
      "EPOCH: 960  / 1000 \t DISC_LOSS: 0.25 \t DISC_ACC: 0.91 \t GEN_LOSS: 3.39\n",
      "EPOCH: 965  / 1000 \t DISC_LOSS: 0.11 \t DISC_ACC: 0.97 \t GEN_LOSS: 3.96\n",
      "EPOCH: 970  / 1000 \t DISC_LOSS: 0.02 \t DISC_ACC: 1.00 \t GEN_LOSS: 5.17\n",
      "EPOCH: 975  / 1000 \t DISC_LOSS: 0.08 \t DISC_ACC: 0.97 \t GEN_LOSS: 5.33\n",
      "EPOCH: 980  / 1000 \t DISC_LOSS: 0.03 \t DISC_ACC: 1.00 \t GEN_LOSS: 4.67\n",
      "EPOCH: 985  / 1000 \t DISC_LOSS: 0.07 \t DISC_ACC: 0.97 \t GEN_LOSS: 4.57\n",
      "EPOCH: 990  / 1000 \t DISC_LOSS: 0.05 \t DISC_ACC: 0.97 \t GEN_LOSS: 4.85\n",
      "EPOCH: 995  / 1000 \t DISC_LOSS: 0.08 \t DISC_ACC: 0.97 \t GEN_LOSS: 6.30\n",
      "EPOCH: 1000 / 1000 \t DISC_LOSS: 0.18 \t DISC_ACC: 0.91 \t GEN_LOSS: 4.86\n",
      "\n",
      "Tota time elapsed:  262.54s\n"
     ]
    }
   ],
   "source": [
    "trainGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "veterinary-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('./Models/generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "homeless-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.save('./Models/discriminator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-clear",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

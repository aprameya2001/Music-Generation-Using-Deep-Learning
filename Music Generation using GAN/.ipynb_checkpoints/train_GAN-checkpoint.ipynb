{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "balanced-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from tensorflow.keras.layers import Activation, Dense, Bidirectional, LSTM, LeakyReLU, BatchNormalization, Reshape, Input\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "documented-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET = './Dataset/jigs'\n",
    "PATH_TO_MODEL = './Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "apparent-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.listdir(PATH_TO_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liable-wellington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n"
     ]
    }
   ],
   "source": [
    "midi_file_list = glob.glob(PATH_TO_DATASET + '/*.mid')\n",
    "print(len(midi_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "casual-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_notes_from_dataset():\n",
    "    notes = []\n",
    "\n",
    "    for i in tqdm(range(len(midi_file_list)), desc='Parsing MIDI files', ncols = 100):\n",
    "        midi_file = midi_file_list[i]\n",
    "\n",
    "        midi = converter.parse(midi_file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        parts = instrument.partitionByInstrument(midi)\n",
    "\n",
    "        if parts: # file has instrument parts\n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open('notes', 'wb') as filepath:\n",
    "        # write notes in binary format to filepath\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "external-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing MIDI files: 100%|█████████████████████████████████████████| 340/340 [00:18<00:00, 18.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of MIDI files in dataset:  340\n",
      "\n",
      "Total number of notes:  85829\n",
      "Total number of unique notes:  75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "notes = extract_notes_from_dataset()\n",
    "\n",
    "print('\\nTotal number of MIDI files in dataset: ', len(midi_file_list))\n",
    "print('\\nTotal number of notes: ', len(notes))\n",
    "print('Total number of unique notes: ', len(set(notes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "northern-awareness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.3.6': 0, '0.3.7': 1, '0.4.7': 2, '0.4.8': 3, '1.4.7': 4, '1.4.7.9': 5, '1.4.8': 6, '10.1.4.6': 7, '10.2.5': 8, '11.2.4.7': 9, '11.2.5.7': 10, '11.2.6': 11, '11.3.6': 12, '2.5.8': 13, '2.5.8.10': 14, '2.5.9': 15, '2.6.9': 16, '3.6.9.11': 17, '3.7.10': 18, '4.7.10': 19, '4.7.10.0': 20, '4.7.11': 21, '4.8.11': 22, '5.8.11': 23, '5.9.0': 24, '6.10.1': 25, '6.9.0.2': 26, '6.9.0.2.3': 27, '6.9.1': 28, '6.9.11': 29, '6.9.11.2': 30, '7.10.1': 31, '7.10.2': 32, '7.11.2': 33, '7.9.1': 34, '8.11.2.4': 35, '8.11.2.4.5': 36, '9.0.3': 37, '9.0.3.5': 38, '9.0.4': 39, '9.1': 40, '9.1.4': 41, '9.11.2.5': 42, 'A3': 43, 'A4': 44, 'A5': 45, 'B-3': 46, 'B-4': 47, 'B-5': 48, 'B3': 49, 'B4': 50, 'B5': 51, 'C#4': 52, 'C#5': 53, 'C#6': 54, 'C4': 55, 'C5': 56, 'C6': 57, 'D4': 58, 'D5': 59, 'D6': 60, 'E-4': 61, 'E-5': 62, 'E4': 63, 'E5': 64, 'F#4': 65, 'F#5': 66, 'F4': 67, 'F5': 68, 'G#3': 69, 'G#4': 70, 'G#5': 71, 'G3': 72, 'G4': 73, 'G5': 74}\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 100\n",
    "\n",
    "unique_notes = sorted(set(notes))\n",
    "unique_notes_count = len(unique_notes)\n",
    "\n",
    "int_to_note = dict((index, note) for index, note in enumerate(unique_notes))\n",
    "note_to_int = dict((note, index) for index, note in enumerate(unique_notes))\n",
    "\n",
    "print(note_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spread-spencer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences:  85729\n",
      "\n",
      "First training sequence:  [66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 66, 50, 11, 53, 50, 50, 35, 66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 53, 59, 64, 16, 66, 64, 59, 35, 53, 50, 44, 41, 53, 64, 45, 66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 66, 50, 11, 53, 50, 50, 35, 66, 64, 41, 53, 53, 53, 66, 64, 41, 53, 53, 53, 53, 59, 64, 16, 66, 64, 59, 35, 53, 50, 44]\n",
      "\n",
      "First target sequence:  41\n"
     ]
    }
   ],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "for i in range(len(notes) - sequence_length):\n",
    "    input_seq = notes[i:i+sequence_length]\n",
    "    target_seq = notes[i+sequence_length]\n",
    "    \n",
    "    input_seq_int = [note_to_int[note] for note in input_seq]\n",
    "    target_seq_int = note_to_int[target_seq]\n",
    "\n",
    "    trainX.append(input_seq_int)\n",
    "    trainY.append(target_seq_int)\n",
    "\n",
    "num_training_seq = len(trainX)\n",
    "\n",
    "print('Number of training sequences: ', num_training_seq)\n",
    "print('\\nFirst training sequence: ', trainX[0])\n",
    "print('\\nFirst target sequence: ', trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "typical-basis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trainX after reshaping (85729, 100, 1)\n",
      "Shape of trainY after one hot encoding:  (85729, 75)\n"
     ]
    }
   ],
   "source": [
    "trainX = np.reshape(trainX, (num_training_seq, sequence_length, 1))\n",
    "print('Shape of trainX after reshaping', trainX.shape)\n",
    "\n",
    "trainX = (trainX - float(unique_notes_count)/2)/(float(unique_notes_count)/2)\n",
    "\n",
    "trainY = to_categorical(trainY)\n",
    "print('Shape of trainY after one hot encoding: ', trainY.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-enemy",
   "metadata": {},
   "source": [
    "# **MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "overall-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 2e-4\n",
    "seq_length = 100\n",
    "latent_dim = 1000\n",
    "seq_shape = (seq_length, 1)\n",
    "generator_loss = []\n",
    "discriminator_loss = []\n",
    "\n",
    "optimizer = Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bulgarian-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=seq_shape, return_sequences=True))\n",
    "    model.add(Bidirectional(LSTM(512)))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print('\\nDiscriminator model: \\n')\n",
    "    model.summary()\n",
    "\n",
    "    input_seq = Input(shape=seq_shape)\n",
    "    output_seq = model(input_seq)\n",
    "\n",
    "    return Model(input_seq, output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "rotary-provider",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-17-207496da151c>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-207496da151c>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    model = Sequential()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def build_generator():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(latent_dim, 1), return_sequences=True))\n",
    "    model.add(Bidirectional(LSTM(512)))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(seq_shape), activation='tanh'))\n",
    "    model.add(Reshape(seq_shape))\n",
    "\n",
    "    print('\\nGenerator model: \\n')\n",
    "    model.summary()\n",
    "    \n",
    "    input_noise = Input(shape=(latent_dim,1))\n",
    "    output_seq = model(input_noise)\n",
    "\n",
    "    return Model(input_noise, output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "french-oxford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discriminator model: \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 1024)              4198400   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,907,457\n",
      "Trainable params: 5,907,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9354022b6098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Build the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Take output from generator after feeding it with noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Take output from generator after feeding it with noise\n",
    "input_seq = Input(shape=(latent_dim,))\n",
    "generated_seq = generator(input_seq)\n",
    "\n",
    "# For combined model only generator should be trained\n",
    "discriminator.trainable = False\n",
    "\n",
    "output_seq = discriminator(generated_seq)\n",
    "\n",
    "combinedModel = Model(input_seq, output_seq)\n",
    "combinedModel.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAN():\n",
    "    global notes, unique_notes_count, trainX, trainY, epochs, batch_size, latent_dim, discriminator_loss, generator_loss\n",
    "    \n",
    "    discriminator_loss = []\n",
    "    generator_loss = []\n",
    "    \n",
    "    real = np.ones((batch_size,1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    start_time = timer()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Choosing a batch of sequences randomly to train model\n",
    "        seq_indexes = np.random.randint(0, len(trainX), batch_size)\n",
    "        real_seq_batch = trainX[seq_indexes]\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        generated_seq_batch = generator.predict(noise)\n",
    "        \n",
    "        # Training discriminator\n",
    "        disc_loss_real = discriminator.train_on_batch(real_seq_batch, real)\n",
    "        disc_loss_fake = discriminator.train_on_batch(generated_seq_batch, fake)\n",
    "        \n",
    "        disc_loss = np.add(disc_loss_real, disc_loss_fake) * 0.5\n",
    "        \n",
    "        # Training generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_loss = combinedModel.train_on_batch(noise, real)\n",
    "        \n",
    "        if epoch == 1 or (epoch+1) % 5 == 0:\n",
    "            print('EPOCH: {:<4} / {} \\t DISC_LOSS: {:.2f} \\t DISC_ACC: {:.2f} \\t GEN_LOSS: {:.2f}'.format(epoch+1, epochs, disc_loss[0], disc_loss[1], gen_loss))\n",
    "            discriminator_loss.append(disc_loss[0])\n",
    "            generator_loss.append(gen_loss)\n",
    "            \n",
    "    end_time = timer()\n",
    "    elapsed_time = '{:.2f}s'.format(end_time - start_time)\n",
    "    print('\\nTota time elapsed: ', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "expired-client",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainGAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3eded5b2bcc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainGAN' is not defined"
     ]
    }
   ],
   "source": [
    "trainGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "veterinary-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('./Models/generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "homeless-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.save('./Models/discriminator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bridal-retention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0032090410004457\n",
      "3.00s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = timer()\n",
    "time.sleep(3)\n",
    "end = timer()\n",
    "\n",
    "print(end - start)\n",
    "x = end-start\n",
    "\n",
    "s = '{:.2f}s'.format(x)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-angola",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
